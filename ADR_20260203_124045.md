# Architecture Decision Record: Translation Pipeline Token Replacement Improvements

**Status**: Accepted
**Date**: 2026-02-03
**Context Tag**: ADR-001-token-replacement-improvements

---

## 1. Executive Summary

Ship boundary-aware regex (4A), fuzzy token matching (4B), and the MBART50 bug fix together as a single coherent commit this week; defer the LLM proofreader skeleton and gender metadata schema extension until the impact of these deterministic fixes is measured against the existing `token_retry_debug.csv` baseline. The debug CSV documents 73 unique sentence-model failures out of 79 tracked cases (92% failure rate among logged entries), which is severe enough to demand immediate action but also severe enough that measurement after the fix is essential before layering on additional complexity.

---

## 2. Context

The translation pipeline uses a token-replacement strategy across three seq2seq model families (Opus-MT, M2M-100, mBART-50) to preserve terminology in EN-FR translation. The current system has four verified issues:

- **Postprocessing uses `str.replace()`** at line 132 of `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\rules_based_replacements\replacements.py`, which is not boundary-aware. Preprocessing uses `\b`-based regex, but the reverse step does not.
- **No fuzzy token recovery.** When the model corrupts tokens (e.g., `NOMENCLATURE0001` becomes `NOMENCLATURE 0001` or `NOMENCLATURE0001s`), the system exhausts 9 retry attempts and falls back to unconstrained translation, losing all terminology control.
- **MBART50 generation_kwargs bug** at line 256 of `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\translate\models.py`: `generation_arguments.update(generation_arguments)` is a self-update no-op -- should be `generation_arguments.update(generation_kwargs)`.
- **`token_retry_debug.csv`** (676 lines, 79 unique sentence-model keys) shows 73 failures and only 6 successes, confirming token corruption is a real, measured production problem -- not a theoretical concern.

The debate centers on whether to also add an LLM proofreader interface and gender metadata now (Waldorf) or measure the impact of deterministic fixes first (Statler).

---

## 3. The Decision

**This week, implement three changes in a single commit:**

1. **4A -- Boundary-aware postprocessing**: Replace `str.replace()` with `re.sub()` using word boundaries in `postprocess_translation()`.
2. **4B -- Fuzzy token matching**: Add a function that attempts to recover corrupted tokens (space insertion, pluralization suffix) before the system falls back to unconstrained translation. Integrate this into `is_valid_translation()` or as a normalization step in `translate_with_retries()`.
3. **MBART50 bug fix**: Change line 256 from `generation_arguments.update(generation_arguments)` to `generation_arguments.update(generation_kwargs)`.

**Defer to a future cycle (after measurement):**

- LLM proofreader interface (Option 2)
- Gender metadata schema extension (Option 4D)

### Rationale

**Why ship 4A and 4B together (siding partly with Waldorf):**

Statler argued for shipping 4A alone and measuring for 2 weeks. But the evidence does not support that delay. The `token_retry_debug.csv` already tells us the corruption rate: 73 out of 79 tracked cases fail all 9 retries. We do not need 2 more weeks of data to know fuzzy matching is needed. The corruption patterns are visible in the CSV -- tokens like `NOMENCLATURE0004` vanish entirely from translations. Fuzzy matching adds approximately 15-20 lines of code and addresses a documented, measured problem. Shipping 4A without 4B would fix postprocessing safety but would not reduce the 92% retry-failure rate that drives fallback to unconstrained translation.

Statler is correct that these are structurally independent changes. But they share the same deployment surface (token replacement pipeline), affect the same code paths, and should be tested together to avoid confounded measurement. One commit, one before/after comparison.

**Why defer the LLM proofreader (siding with Statler):**

Waldorf argued for building a disabled LLM proofreader skeleton now to avoid "retrofitting costs." This argument is weak for three reasons:

1. The proofreader is an additive post-processing step. The `lexical_constraints_options.md` document explicitly states it "doesn't change existing pipeline" (Risk Level: Low). There is no architectural lock-in to retrofit around. The proofreader will accept a translated string and return a corrected string -- that interface will not change regardless of when it is built.
2. The deterministic fixes (4A, 4B) may substantially reduce the problem the proofreader would solve. Until we measure the post-fix fallback rate, we do not know the remaining error budget the proofreader needs to address.
3. Building speculative interfaces without consumers violates the project's demonstrated incremental style (visible in the git history of small, measured changes).

**Why defer gender metadata (siding with Statler):**

Waldorf's "5 hours now vs 40-60 hours later" estimate is not supported by evidence. The current terminology JSON (`C:\Users\CARRK\Documents\Repositories\AI\Data\preferential_translations.json`, 2656 lines, ~350-400 terms) uses a simple `{french_term: english_string_or_dict}` schema. Adding a `"gender"` field is a schema change, not a refactoring project. The cost of adding it later is approximately equal to adding it now, because:

- No consumer code exists that would need to be retroactively updated.
- The `get_translation_value()` function in `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\rules_based_replacements\token_utils.py` (line 22-25) already handles both dict and string values, so the schema is already extensible.
- There is no code path that would break or need "emergency refactoring" when gender is eventually added.

However, Waldorf raises a valid point that gender metadata will eventually be needed. This is acknowledged and scheduled as a follow-up, not dismissed.

---

## 4. The Compromise

| Concern | Statler (Speed) | Waldorf (Safety) | Decision |
|---------|-----------------|-------------------|----------|
| 4A + 4B together? | Ship 4A alone, measure | Ship together, mandatory | **Ship together** -- debug CSV already provides the measurement baseline |
| LLM proofreader? | Wait until deterministic wins exhausted | Build skeleton now | **Wait** -- no architectural lock-in to prevent, measure first |
| Gender metadata? | No consumer, YAGNI | 5 hours prevents 40 hours later | **Wait** -- schema is already extensible, cost is symmetric |
| MBART50 bug? | Both agree | Both agree | **Fix immediately** |
| Measurement? | Measure before next step | Parallel validation | **Sequential** -- run the same test set against pre/post fix, then decide next step |

---

## 5. Consequences

### What we gain
- The MBART50 model starts actually using custom generation parameters (beam settings, length penalties), meaning all 9 retry configurations will produce meaningfully different outputs instead of repeating the same default.
- Boundary-aware postprocessing eliminates the class of bugs where token replacement corrupts adjacent text.
- Fuzzy matching recovers a significant portion of the 73 currently-failing sentence-model cases, reducing fallback to unconstrained translation.
- A clear before/after measurement on the existing test set, using the debug CSV as a baseline.

### What we defer
- LLM proofreader: deferred until post-fix fallback rate is measured. If fallback rate remains above 10% after fixes, the proofreader becomes the next priority.
- Gender metadata: deferred until a consumer exists. The schema is extensible now; adding gender is a data task, not an architecture task.
- Option 4C (similarity_vs_target for model selection): out of scope for this cycle, but worth evaluating after the current fixes land.

### What we monitor
- **Fallback rate**: After deploying 4A+4B+MBART fix, run the same test set used to generate `token_retry_debug.csv`. Compare the new failure count against the current 73/79 baseline.
- **Threshold for next action**: If post-fix fallback rate is above 10%, begin LLM proofreader implementation. If below 10%, focus on gender metadata and Option 4C.

---

## 6. Action Items

Ordered by implementation sequence. All changes should be tested against the same corpus that generated `token_retry_debug.csv` before and after.

1. **Fix the MBART50 generation_kwargs bug** -- In `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\translate\models.py`, line 256, change `generation_arguments.update(generation_arguments)` to `generation_arguments.update(generation_kwargs)`. This is a 1-word fix. Estimated effort: 5 minutes.

2. **Implement boundary-aware postprocessing (4A)** -- In `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\rules_based_replacements\replacements.py`, in the `postprocess_translation()` function at line 132, replace `result_text = result_text.replace(token, replacement)` with a `re.sub()` call using `r'\b' + re.escape(token) + r'\b'` as the pattern. The `re` module is already imported at the top of this file. Estimated effort: 15 minutes.

3. **Implement fuzzy token matching (4B)** -- Add a function (approximately 15-20 lines) to `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\rules_based_replacements\replacements.py` that attempts to find corrupted tokens using two patterns: (a) optional whitespace inserted between the category prefix and the numeric suffix (e.g., `NOMENCLATURE\s+0001`), and (b) trailing pluralization suffix (e.g., `NOMENCLATURE0001s`). Call this function from within `postprocess_translation()` when a token is not found via exact match. If a corrupted match is found, normalize it back to the clean token before performing replacement. Estimated effort: 30 minutes.

4. **Integrate fuzzy matching into the validation path** -- In `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\translate\models.py`, update `is_valid_translation()` (lines 365-374) so that before declaring a token missing, it calls the fuzzy matcher. If the fuzzy matcher finds a corrupted form, the text should be normalized in-place so downstream postprocessing can proceed. This avoids burning 9 retries on tokens that are present but corrupted. Estimated effort: 20 minutes.

5. **Run the existing test set** -- Translate the same inputs used to generate `token_retry_debug.csv` with the patched code. Produce a new debug CSV. Compare failure counts. Record the before/after numbers. Estimated effort: depends on GPU availability, but the comparison analysis is 15 minutes.

6. **Decision gate after measurement** -- If post-fix fallback rate exceeds 10%: prioritize LLM proofreader (Option 2) in the next cycle. If below 10%: prioritize gender metadata (Option 4D) and model selection improvement (Option 4C) instead.

**Total estimated implementation effort for items 1-4: under 90 minutes.**

---

## 7. Files Referenced

| File | Role in this decision |
|------|----------------------|
| `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\translate\models.py` | MBART50 bug (line 256), `is_valid_translation()` (lines 365-374), retry logic |
| `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\rules_based_replacements\replacements.py` | `postprocess_translation()` (line 132), boundary-aware fix, fuzzy matching |
| `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\rules_based_replacements\token_utils.py` | Token utilities, schema already extensible (lines 22-25) |
| `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\token_retry_debug.csv` | Baseline measurement: 73/79 failures, 676 lines |
| `C:\Users\CARRK\Documents\Repositories\AI\Data\preferential_translations.json` | Terminology JSON, 2656 lines, ~350-400 terms |
| `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\lexical_constraints_options.md` | Options analysis document |
| `C:\Users\CARRK\Documents\Repositories\AI\Pipeline\config.py` | `PREFERENTIAL_JSON_PATH` and model variant configuration |
